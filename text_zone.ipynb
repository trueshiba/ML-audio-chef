{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 5990: Advanced Machine Learning Final Project\n",
    "\n",
    "Mish Wilson and Cooper Sullivan\n",
    "\n",
    "10/23/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "from IPython.display import Audio\n",
    "from numpy.fft import fft, ifft\n",
    "from os.path import dirname, join as pjoin\n",
    "from scipy.io import wavfile\n",
    "import scipy.io \n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import math, random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see current working directory w correct files listed\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if we can read single file and get samplerate and data attributes\n",
    "data_dir = pjoin(os.curdir, '.\\clips_rd', 'aloe')\n",
    "wav_fname = pjoin(data_dir, 'aloe_1_02.wav')\n",
    "samplerate, data = wavfile.read(wav_fname)\n",
    "print(f\"length = {data.shape[0] / samplerate}s\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read single audio clip with regular IO function and call as audio\n",
    "Fs, data = read('./clips_rd/aloe/aloe_1_02.wav')\n",
    "\n",
    "Audio(data, rate=Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveform to better understand visualness of audio\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(data)\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Waveform of Test Audio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each audio sample in the dataset such as its filename, its class label, the ‘fold’ sub-folder location, and so on.\n",
    "\n",
    "We will create a CSV to store path data for loading. Format: relative_path, classID, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display food labels\n",
    "path = os.getcwd() + \"/clips_rd\" \n",
    "food_categories = os.listdir(path) \n",
    "print(\"Files and directories in '\", path, \"' :\")  \n",
    "print(food_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create general CSV for label referencing\n",
    "\n",
    "data = [['class', 'classID']]\n",
    "for i, food in enumerate(food_categories):\n",
    "    data.append([food, i+1])\n",
    "\n",
    "# Create file path for the CSV file\n",
    "csv_file_path = 'classID.csv'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    # Create a csv.writer object\n",
    "    writer = csv.writer(file)\n",
    "    # Write data to the CSV file\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata CSV file to store audio paths as well as ID and labels\n",
    "\n",
    "data = [['relative_path', 'classID', 'label']]\n",
    "for i, food in enumerate(food_categories):\n",
    "    path = os.getcwd() + \"\\\\clips_rd\\\\\" + food\n",
    "    wavs = os.listdir(path)\n",
    "    for wav in wavs:\n",
    "        data.append([f\"./clips_rd/{food}/{wav}\",i+1, food])\n",
    "\n",
    "print(data)\n",
    "\n",
    "# File path for the CSV file\n",
    "csv_file_path = 'metadeta.csv'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    # Create a csv.writer object\n",
    "    writer = csv.writer(file)\n",
    "    # Write data to the CSV file\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranforming Data / Audio Files\n",
    "\n",
    "Here we transform the data so that its in a format the model expects. This can be done during runtime. \n",
    "\n",
    "Audio files tend to be large- so we're keeping only the audio file names in our taining data.\n",
    "\n",
    "Then at run time, as we train the model we'll load the audio data for that bath and then process it. This way we keep audiodata for only one batch in memory at a time reducing the overall strain on system by loading everything in all at once and having to store it all.\n",
    "\n",
    "Transforms:\n",
    "- Load Audio file\n",
    "- Resample and convert to stereo or mono\n",
    "    - Testing on aloe- so far all the files seem to have 1 channel so no need to convert them!\n",
    "- resize to fixed length so all the audios are the same length\n",
    "- audio augmentation time shift\n",
    "- convert to mel spectogram\n",
    "- spectogram augmentation. SPECAUGMENTATING TIME!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential audio processing libraries:\n",
    "- librosa\n",
    "    - slower that pytorch\n",
    "    - seems to be used for smaller datasets?\n",
    "    - usually used for making spectograms- but since pytorch came out with torchaudio it isn't used as much by new learners- usually used by people who are used to the library. \n",
    "- pytorch (torchaudio)\n",
    "    - pretty fast\n",
    "    - may be better for such a large data set\n",
    "    - well documented (in my opinion)\n",
    "    - we'll be using pytorch anyway to processed the spectrogram- so we probably should just use torchaudio??\n",
    "- audio flux\n",
    "    - super fast\n",
    "    - fairly new\n",
    "    - less digital examples to follow when learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "\n",
    "def open_audio(audio_file):\n",
    "    signal, sample_rate = torchaudio.load(audio_file)\n",
    "    return signal, sample_rate\n",
    "\n",
    "df = pd.read_csv('metadeta.csv')\n",
    "\n",
    "# Check if audio file backend exists\n",
    "# Output should be ['soundfile']. If output is [] then redownload libraries in specific order.\n",
    "print(torchaudio.list_audio_backends())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Open Audio, for now we're just using aloe\n",
    "audio_files = []\n",
    "\n",
    "# Keep track incase not all files are correctly loaded\n",
    "success = 0\n",
    "fail = 0\n",
    "\n",
    "for audio in df[df['label'] == 'aloe']['relative_path']:\n",
    "    try:\n",
    "        audio_files.append(open_audio(audio))\n",
    "        success += 1\n",
    "    except RuntimeError:\n",
    "        fail += 1\n",
    "        print(f\"Skipping file: {audio}\")\n",
    "print(f\"Successes: {success}\\nFails: {fail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code to ensure audio loaded correctly\n",
    "\n",
    "test_signal, test_sr = audio_files[0]\n",
    "\n",
    "# for audio in audio_files:\n",
    "#     print(audio[1])\n",
    "\n",
    "len(test_signal) / test_sr\n",
    "test_signal.shape[0]\n",
    "\n",
    "#154293 // 1000 * 3000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Before Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sr = audio_files[1]\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "\n",
    "waveform = waveform.numpy()\n",
    "num_channels, num_frames = waveform.shape\n",
    "time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "ax = axs\n",
    "ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "ax.grid(True)\n",
    "ax.set_xlim([0, time_axis[-1]])\n",
    "ax.set_title(\"Original Waveform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Sample Rate\n",
    "Just looping through and making sure all the audio files have the same sample rate! Majority seem to be 44100Hz- but just to make sure I'll loop through to catch any that aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample takes in audio file (aud) and desired sample rate (newsr) and ensures they match\n",
    "def resample(aud, newsr):\n",
    "    signal, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      return audio\n",
    "        \n",
    "    # Resample channel\n",
    "    re_signal = torchaudio.transforms.Resample(sr, newsr)(signal[:1,:])\n",
    "    \n",
    "\n",
    "    return ((re_signal, newsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and resample all audio so its on the same page (44100Hz)\n",
    "t_audio_files = []\n",
    "for audio in audio_files:\n",
    "    t_audio_files.append(resample(audio, 44100))\n",
    "\n",
    "audio_files = t_audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing Audio Lengths\n",
    "\n",
    "Resizing  samples to be the same length. Found that they are infact different lengths ranging between 2 and 4? I think I'll just average it out to 3 seconds each. Padding it out with silence if its 2 seconds and truncating if its 4? We can change this if this is bad.\n",
    "\n",
    "Math to find length of clips: \n",
    "Sample Rate // 44100 Hz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(aud, max_ms):\n",
    "    signal, sample_rate = aud\n",
    "    num_rows, signal_len = signal.shape\n",
    "\n",
    "    # calculate what would make audio max_ms long\n",
    "    max_len = sample_rate//1000 * max_ms\n",
    "\n",
    "    if (signal_len > max_len):\n",
    "      # Truncate signal\n",
    "      signal = signal[:,:max_len]\n",
    "\n",
    "    elif (signal_len < max_len):\n",
    "      # Pad audio with silence instead. At beginning and end.\n",
    "      pad_begin_len = random.randint(0, max_len - signal_len)\n",
    "      pad_end_len = max_len - signal_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      signal = torch.cat((pad_begin, signal, pad_end), 1)\n",
    "      \n",
    "    return (signal, sample_rate)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and resize all audio so they're all 3 seconds long (3000 miliseconds)\n",
    "t_audio_files = []\n",
    "for audio in audio_files:\n",
    "    t_audio_files.append(resize(audio, 3000))\n",
    "\n",
    "audio_files = t_audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Shift\n",
    "Not sure if we need to do this augmentation shince I think we have a pretty diverse dataset? But We can return to this incase we do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Spectrogram Creation.\n",
    "\n",
    "Time to creat some spectrograms! Specifically Mel Spectrograms instead of jsut Spectrograms as they are better for training models it seems.\n",
    "\n",
    "\"The mel-spectrogram, based on the auditory-based mel-frequency scale, provides better resolution for lower frequencies than the spectrogram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_mels : Number of mel filterbanks\n",
    "# n_fft : Size of FFT\n",
    "# hop_len : Length of hop between STFT windows\n",
    "def spectrogram(audio, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    signal, sample_rate = audio\n",
    "    top_db = 80\n",
    "\n",
    "    # spectrogram shape is [channel, n_mels, time]\n",
    "    spec = transforms.MelSpectrogram(sample_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(signal)\n",
    "\n",
    "    # Convert to decibels \n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = spectrogram(audio_files[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title='Spectrogram'):\n",
    "    # Convert the tensor to a numpy array and squeeze to 2D\n",
    "    spec = spec.squeeze().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spec, aspect='auto', origin='lower')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (frames)')\n",
    "    plt.ylabel('Mel frequency bins')\n",
    "    return plt\n",
    "\n",
    "# plot_spectrogram(spec, title='Spectrogram')\n",
    "\n",
    "spec = spectrogram(audio_files[1])\n",
    "waveform, sr = audio_files[1]\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "waveform = waveform.numpy()\n",
    "num_channels, num_frames = waveform.shape\n",
    "time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "ax.grid(True)\n",
    "ax.set_xlim([0, time_axis[-1]])\n",
    "ax.set_title(\"Original Waveform\")\n",
    "\n",
    "s_spec = spec.squeeze().numpy()\n",
    "\n",
    "ax = axs[1]\n",
    "ax.imshow(s_spec, aspect='auto', origin='lower')\n",
    "ax.set_title(\"Spectrogram\")\n",
    "ax.set_xlabel('Time (frames)')\n",
    "ax.set_ylabel('Mel frequency bins')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPEC Augmentation\n",
    "Time and Frequency Masking!\n",
    "\n",
    "fREQUENCY mASK: Randomly Mask out a range of consecutive frequencies by adding horizonatal bars to our spectro gram\n",
    "\n",
    "Time Mask: Randomly block out ranges of time from the spectrogram by using vertical bars.\n",
    "\n",
    "We do this to prevent voerfitting and to help the model generalize better. The masked sections get replaced with a mean value :0 Which is pretty cool!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.shape\n",
    "# ..., number of mels, number of steps (time)\n",
    "# Documentation literally says the first index is '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaulting to 10% and only one mask until we deiscuss what would \n",
    "# be most optional :)\n",
    "def spec_mask(spec, max_mask_pct=0.1, num_freq_masks=1, num_time_masks=1):\n",
    "    _, num_mels, num_steps = spec.shape\n",
    "    \n",
    "    # Get the average to replace the blocked zones\n",
    "    mask_val = spec.mean()\n",
    "    augment_spec = spec\n",
    "    \n",
    "    # Add frquency mask- will look like a horizonal bar\n",
    "    freq_mask_param = max_mask_pct * num_mels\n",
    "    for _ in range(num_freq_masks):\n",
    "        augment_spec = transforms.FrequencyMasking(freq_mask_param)(augment_spec, mask_val)\n",
    "\n",
    "    # Add time mask- will look like a vertical bar\n",
    "    time_mask_param = max_mask_pct * num_steps\n",
    "    for _ in range(num_time_masks):\n",
    "        augment_spec = transforms.TimeMasking(time_mask_param)(augment_spec,mask_val)\n",
    "\n",
    "    return augment_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = spectrogram(audio_files[1])\n",
    "waveform, sr = audio_files[1]\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "waveform = waveform.numpy()\n",
    "num_channels, num_frames = waveform.shape\n",
    "time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "ax.grid(True)\n",
    "ax.set_xlim([0, time_axis[-1]])\n",
    "ax.set_title(\"Original Waveform\")\n",
    "\n",
    "s_spec = spec.squeeze().numpy()\n",
    "\n",
    "ax = axs[1]\n",
    "ax.imshow(s_spec, aspect='auto', origin='lower')\n",
    "ax.set_title(\"Spectrogram\")\n",
    "ax.set_xlabel('Time (frames)')\n",
    "ax.set_ylabel('Mel frequency bins')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = spectrogram(audio_files[1])\n",
    "spec = spec_mask(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sr = audio_files[1]\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "waveform = waveform.numpy()\n",
    "num_channels, num_frames = waveform.shape\n",
    "time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "ax.grid(True)\n",
    "ax.set_xlim([0, time_axis[-1]])\n",
    "ax.set_title(\"Original Waveform\")\n",
    "\n",
    "s_spec = spec.squeeze().numpy()\n",
    "\n",
    "ax = axs[1]\n",
    "ax.imshow(s_spec, aspect='auto', origin='lower')\n",
    "ax.set_title(\"Spectrogram\")\n",
    "ax.set_xlabel('Time (frames)')\n",
    "ax.set_ylabel('Mel frequency bins')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader for Model\n",
    "- dataset Object that tranforms the audio- preping each data item\n",
    "- DataLoader from torch- to fecth the sata items and packages them to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together audio opening function as well as audio transformation functions into a convenient class\n",
    "class AudioFuncs():\n",
    "    \n",
    "    def open_audio(audio_file):\n",
    "        signal, sample_rate = torchaudio.load(audio_file)\n",
    "        return signal, sample_rate\n",
    "\n",
    "\n",
    "    # Resample takes in audio file (aud) and desired sample rate (newsr) and ensures they match\n",
    "    def resample(aud, newsr):\n",
    "        signal, sr = aud\n",
    "\n",
    "        if (sr == newsr):\n",
    "            return audio\n",
    "            \n",
    "        # Resample channel\n",
    "        re_signal = torchaudio.transforms.Resample(sr, newsr)(signal[:1,:])\n",
    "    \n",
    "        return ((re_signal, newsr))\n",
    "\n",
    "\n",
    "    # Resize audio to be of specific time length (max_ms)\n",
    "    def resize(aud, max_ms):\n",
    "        signal, sample_rate = aud\n",
    "        num_rows, signal_len = signal.shape\n",
    "\n",
    "        # calculate what would make audio max_ms long\n",
    "        max_len = sample_rate//1000 * max_ms\n",
    "\n",
    "        if (signal_len > max_len):\n",
    "            # Truncate signal\n",
    "            signal = signal[:,:max_len]\n",
    "\n",
    "        elif (signal_len < max_len):\n",
    "            # Pad audio with silence instead. At beginning and end.\n",
    "            pad_begin_len = random.randint(0, max_len - signal_len)\n",
    "            pad_end_len = max_len - signal_len - pad_begin_len\n",
    "\n",
    "            # Pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "            signal = torch.cat((pad_begin, signal, pad_end), 1)\n",
    "        \n",
    "        return (signal, sample_rate)\n",
    "\n",
    "\n",
    "    # Rechannel audio\n",
    "    def rechannel(audio, max_channels):\n",
    "        \n",
    "        signal, sr = audio\n",
    "\n",
    "        # Verify audio isn't already correct number of channels \n",
    "        if (signal.shape[0] == max_channels):\n",
    "            return audio\n",
    "        \n",
    "        # If audio channels > max_channels -> Reduce channels to max_channels\n",
    "        if (signal.shape[0] > max_channels):\n",
    "            new_signal = signal[:max_channels, :]\n",
    "            \n",
    "        else:\n",
    "            # Add bonus channels to get to max_channels\n",
    "            new_signal = signal\n",
    "            \n",
    "            # Loop through initially duplicating/ doubling channels up to point before overflow\n",
    "            while (new_signal.shape[0] + signal.shape[0]) < max_channels:\n",
    "                new_signal = torch.cat([new_signal, signal])\n",
    "            # After reaching number of channels before doubling would overflow just add single channel until reaching max_channels\n",
    "            while new_signal.shape[0] < max_channels:\n",
    "                new_signal = torch.cat([new_signal, signal[:1, :]])\n",
    "\n",
    "        return (new_signal, sr)\n",
    "\n",
    "\n",
    "    # Creates spectrogram for given audio sample with specified mel bins and Fourier transformations.\n",
    "    def spectrogram(audio, n_mels=128, n_fft=1024, hop_len=None):\n",
    "        signal, sample_rate = audio\n",
    "        top_db = 80\n",
    "\n",
    "        # spectrogram shape is [channel, n_mels, time]\n",
    "        spec = transforms.MelSpectrogram(sample_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(signal)\n",
    "\n",
    "        # Convert to decibels \n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec\n",
    "    \n",
    "\n",
    "    # Visualize given spectrogram (spec) with matplotlib\n",
    "    def plot_spectrogram(spec, title='Spectrogram'):\n",
    "        # Convert the tensor to a numpy array and squeeze to 2D\n",
    "        spec = spec.squeeze().numpy()\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.imshow(spec, aspect='auto', origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Time (frames)')\n",
    "        plt.ylabel('Mel frequency bins')\n",
    "        return plt\n",
    "    \n",
    "\n",
    "    # Mask a percentage of the given spectrogram with averaging data to in theory reduce overfitting when training data. \n",
    "    # Defaulting to 10% and only one mask until we deiscuss what would be most optional for generalizing :)\n",
    "    def spec_mask(spec, max_mask_pct=0.1, num_freq_masks=1, num_time_masks=1):\n",
    "        _, num_mels, num_steps = spec.shape\n",
    "        \n",
    "        # Get the average to replace the blocked zones\n",
    "        mask_val = spec.mean()\n",
    "        augment_spec = spec\n",
    "        \n",
    "        # Add frquency mask- will look like a horizonal bar\n",
    "        freq_mask_param = max_mask_pct * num_mels\n",
    "        for _ in range(num_freq_masks):\n",
    "            augment_spec = transforms.FrequencyMasking(freq_mask_param)(augment_spec, mask_val)\n",
    "\n",
    "        # Add time mask- will look like a vertical bar\n",
    "        time_mask_param = max_mask_pct * num_steps\n",
    "        for _ in range(num_time_masks):\n",
    "            augment_spec = transforms.TimeMasking(time_mask_param)(augment_spec,mask_val)\n",
    "\n",
    "        return augment_spec\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test audio channel splitting functionality\n",
    "test_signal, test_sr = audio_files[0]\n",
    "print(f\"Test audio with {test_signal.shape[0]} channels\")\n",
    "\n",
    "new_audio = AudioFuncs.rechannel(audio_files[0], 2)\n",
    "print(f\"New audio with {new_audio[0].shape[0]} channels\")\n",
    "\n",
    "new_audio_2 = AudioFuncs.rechannel(new_audio, 5)\n",
    "print(f\"New audio with {new_audio_2[0].shape[0]} channels\")\n",
    "\n",
    "new_audio_3 = AudioFuncs.rechannel(new_audio_2, 2)\n",
    "print(f\"New audio with {new_audio_3[0].shape[0]} channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodLoader(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, target_transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        #self.transform = transform\n",
    "        #self.target_transform = target_transform\n",
    "\n",
    "        self.duration = 3000\n",
    "        self.sample_rate = 44100\n",
    "        self.channel = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Non-CNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into tensorflow training and testing dataset after split\n",
    "# Not attempting to load all data in at once due to the fact theres 11k of these audio files. \n",
    "# Will instead split dataset in half and use percentage out of that to train an initial model and see how it goes.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "audio_files = []\n",
    "audio_labels = []\n",
    "\n",
    "# Keep track incase not all files are correctly loaded\n",
    "success = 0\n",
    "fail = 0\n",
    "\n",
    "# Get every 4th row in dataset\n",
    "for row in df[::5].iterrows():\n",
    "    try:\n",
    "\n",
    "        audio_path = row[1][0]\n",
    "        audio_label = row[1][1]\n",
    "\n",
    "        audio = open_audio(audio_path)\n",
    "        trans_audio = AudioFuncs.resample(audio, 44100)\n",
    "        trans_audio = AudioFuncs.rechannel(trans_audio, 1)\n",
    "        trans_audio = AudioFuncs.resize(trans_audio, 3000)\n",
    "        trans_audio = AudioFuncs.spectrogram(trans_audio)\n",
    "        #s = np.mean(.T,axis=0)\n",
    "        audio_files.append(trans_audio)\n",
    "        audio_labels.append(audio_label)\n",
    "\n",
    "        success += 1\n",
    "    except RuntimeError:\n",
    "        fail += 1\n",
    "        print(f\"Skipping file: {audio}\")\n",
    "print(f\"Successes: {success}\\nFails: {fail}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression obj and do partial fit with individual batches.\n",
    "x_train, x_test, y_train, y_test = train_test_split(audio_files, audio_labels, test_size=0.20, random_state=0)\n",
    "\n",
    "models = [SVC(kernel='linear'), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "scores = []\n",
    "\n",
    "for model in models:\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "    scores.append((type(model).__name__, (f'{100*score:.2f}'))\n",
    "    \n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, prediction)\n",
    "plt.imshow(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
